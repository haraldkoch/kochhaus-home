---
version: "3"

vars:
  TIMENOW:
    sh: date +%Y%m%d-%H%M%S
  # renovate: datasource=github-releases depName=prometheus-operator/prometheus-operator
  PROMETHEUS_OPERATOR_VERSION: v0.84.0

tasks:

  verify:
    desc: Verify flux meets the prerequisites
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmd: flux --context {{.cluster}} check --pre
    requires:
      vars: ["cluster"]

  install:
    desc: Install Flux into your cluster
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      # Install essential Prometheus Operator CRDs
      - kubectl --context {{.cluster}} apply --server-side --filename https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/{{.PROMETHEUS_OPERATOR_VERSION}}/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
      - kubectl --context {{.cluster}} apply --server-side --filename https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/{{.PROMETHEUS_OPERATOR_VERSION}}/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
      - kubectl --context {{.cluster}} apply --server-side --filename https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/{{.PROMETHEUS_OPERATOR_VERSION}}/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml
      - kubectl --context {{.cluster}} apply --server-side --filename https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/{{.PROMETHEUS_OPERATOR_VERSION}}/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
      # Install Flux
      - kubectl --context {{.cluster}} apply --kustomize {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap
      - cat {{.SOPS_AGE_KEY_FILE}} | kubectl --context {{.cluster}} -n flux-system create secret generic sops-age --from-file=age.agekey=/dev/stdin
      - sops --decrypt {{.KUBERNETES_DIR}}/{{.cluster}}/flux/vars/cluster-secrets.sops.yaml | kubectl --context {{.cluster}} apply -f -
      - kubectl --context {{.cluster}} apply -f {{.KUBERNETES_DIR}}/{{.cluster}}/flux/vars/cluster-settings.yaml
      - kubectl --context {{.cluster}} apply --kustomize {{.KUBERNETES_DIR}}/{{.cluster}}/flux/config
    requires:
      vars: ["cluster"]
    preconditions:
      - { msg: "Age key file is not found. Did you forget to create it?", sh: "test -f {{.SOPS_AGE_KEY_FILE}}" }
    vars:
      SOPS_AGE_KEY_FILE: ~/.config/sops/age/keys.txt

  reconcile:
    desc: Force update Flux to pull in changes from your Git repository
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmd: flux --context {{.cluster}} reconcile -n flux-system kustomization cluster --with-source
    requires:
      vars: ["cluster"]

  list-dockerhub:
    desc: What dockerhub images are running in my cluster
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - kubectl --context {{.cluster}} get pods --all-namespaces -o=jsonpath="{range .items[*]}{'\n'}{range .spec.containers[*]}{.image}{'\n'}{end}{end}" | sort | uniq | grep -Ev 'quay|gcr|ghcr|ecr|us-docker|registry.k8s' | grep -Ev 'bitnami|rook|intel|grafana' |  sed -e 's/docker\.io\///g' | sort | uniq
    requires:
      vars: ["cluster"]

  delete-failed-pods:
    desc: Deletes failed pods
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - kubectl --context {{.cluster}} delete pods --field-selector status.phase=Failed -A --ignore-not-found=true
    requires:
      vars: ["cluster"]

  delete-jobs:
    desc: Delete all jobs
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - kubectl --context {{.cluster}} delete job -A --all
    requires:
      vars: ["cluster"]

  hr-restart:
    desc: Restart all failed Helm Releases
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - kubectl --context {{.cluster}} get hr --all-namespaces | grep False | awk '{print $2, $1}' | xargs -L1 bash -c 'flux --context {{.cluster}} suspend hr $0 -n $1'
      - kubectl --context {{.cluster}} get hr --all-namespaces | grep False | awk '{print $2, $1}' | xargs -L1 bash -c 'flux --context {{.cluster}} resume hr $0 -n $1'
    requires:
      vars: ["cluster"]

  wait-pod-pending:
    aliases: [waitp]
    internal: true
    desc: Wait for a job's pod to change its status to pending
    vars:
      CLUSTER: '{{ or .CLUSTER (fail "Missing `CLUSTER` environment variable!") }}'
      NAME: '{{ or .NAME (fail "Missing `NAME` environment variable!") }}'
      NS: '{{ .NS | default "default" }}'
    cmds:
      - until [[ $(kubectl --context {{.CLUSTER}} -n "{{.NS}}" get pod "{{.NAME}}" -o jsonpath='{.items[*].status.phase}') == "Pending" ]]; do sleep 1; done

  wait-pod-running:
    aliases: [waitp]
    internal: true
    desc: Wait for a job's pod to change its status to pending
    vars:
      CLUSTER: '{{ or .CLUSTER (fail "Missing `CLUSTER` environment variable!") }}'
      NAME: '{{ or .NAME (fail "Missing `NAME` environment variable!") }}'
      NS: '{{ .NS | default "default" }}'
    cmds:
      - until [[ $(kubectl --context {{.CLUSTER}} -n "{{.NS}}" get pod "{{.NAME}}" -o jsonpath='{.items[*].status.phase}') == "Running" ]]; do sleep 1; done

  wait-finish:
    internal: true
    desc: Wait for a job's pod to change its status to pending
    vars:
      CLUSTER: '{{ or .CLUSTER (fail "Missing `CLUSTER` environment variable!") }}'
      NAME: '{{ or .NAME (fail "Missing `NAME` environment variable!") }}'
      NS: '{{ .NS | default "default" }}'
      # TYPE: '{{ .TYPE | default "job" }}'
      # WAIT_ARGS: '{{.WAIT_ARGS | default "echo \"{{.NAME}} is still running, logs:\" && kubectl -n {{.NS}} logs {{.NAME}} --since 2s -f;"}}'
    cmds:
      - |-
        until kubectl --context {{.CLUSTER}} -n {{.NS}} wait {{.NAME}} --for condition=complete --timeout=2s; do
          echo "{{.NAME}} is still running, logs:" && kubectl --context {{.CLUSTER}} -n {{.NS}} logs {{.NAME}} --since 2s -f || true;
        done

  iperf2:
    desc: Start a iperf2 server on node SERVER_NODE, and iperf2 client on node CLIENT_NODE, to benchmark network performance.
    dir: "{{.ROOT_DIR}}/.taskfiles/Cluster/iperf2"
    vars: &iperf2-vars
      SERVER_NAME: &iperf2-server-name 'iperf2-server-{{- .TIMENOW -}}'
      SERVER_NS: &iperf2-server-ns '{{ .SERVER_NS | default "default" }}'
      CLIENT_NAME: &iperf2-client-name 'iperf2-client-{{- .TIMENOW -}}'
      CLIENT_NS: &iperf2-client-ns '{{ .CLIENT_NS | default "default" }}'
      CLUSTER: '{{ .cluster }}'
      CLUSTER_DOMAIN: '{{ .CLUSTER_DOMAIN | default "cluster.local" }}'
      SERVER_PORT: '{{ .SERVER_PORT | default "5001" }}'
      SERVER_NODE: '{{ or .SERVER_NODE (fail "Missing `SERVER_NODE` environment variable!") }}'
      CLIENT_NODE: '{{ or .CLIENT_NODE (fail "Missing `CLIENT_NODE` environment variable!") }}'
      SERVER_ARGS: '{{ .SERVER_ARGS | default "" }}'
      CLIENT_ARGS: '{{ .CLIENT_ARGS | default "" }}'
    env: *iperf2-vars
    cmds:
      - cat ./ServerJob.tmpl.yaml | envsubst | kubectl --context {{.cluster}} apply -f -
      - defer: cat ./ServerJob.tmpl.yaml | envsubst | kubectl --context {{.cluster}} delete -f -
      - task: wait-pod-running
        vars:
          NAME: '-l job-name={{.SERVER_NAME}}'
          NS: '{{.SERVER_NS}}'
      - cat ./ClientJob.tmpl.yaml | envsubst | kubectl --context {{.cluster}} apply -f -
      - defer: cat ./ClientJob.tmpl.yaml | envsubst | kubectl --context {{.cluster}} delete -f -
      - task: wait-finish
        vars:
          NAME: 'jobs/{{.CLIENT_NAME}}'
          NS: '{{.CLIENT_NS}}'
    requires:
      vars: ["cluster"]
