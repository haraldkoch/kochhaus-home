---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:

  browse-pvc:
    desc: Browse PersistentVolumeClaims
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        ns: Namespace to browse PersistentVolumeClaims in (default: default)
        claim: PersistentVolumeClaim to browse (required)
    interactive: true
    cmd: |
      kubectl --context {{.cluster}} run -n {{.ns}} debug-{{.claim}} -i --tty --rm --image=null --privileged --overrides='
        {
          "apiVersion": "v1",
          "spec": {
            "containers": [
              {
                "name": "debug",
                "image": "ghcr.io/haraldkoch/ubuntu:rolling",
                "command": ["/bin/bash"],
                "stdin": true,
                "stdinOnce": true,
                "tty": true,
                "volumeMounts": [
                  {
                    "name": "config",
                    "mountPath": "/config"
                  }
                ]
              }
            ],
            "volumes": [
              {
                "name": "config",
                "persistentVolumeClaim": {
                  "claimName": "{{.claim}}"
                }
              }
            ],
            "restartPolicy": "Never"
          }
        }'
    vars:
      ns: '{{.ns | default "default"}}'
    requires:
      vars: ["cluster", "claim"]

  drain:
    desc: Drain a node
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        node: Node to drain (required)
    cmd: kubectl --context {{.cluster}} drain {{.node}} --ignore-daemonsets --delete-local-data --force
    requires:
      vars: ["cluster", "node"]

  delete-failed-pods:
    desc: Deletes pods with a fucked status
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - for: ["Evicted", "Failed", "Succeeded"]
        cmd: kubectl --context {{.cluster}} delete pods --field-selector status.phase={{.ITEM}} -A --ignore-not-found=true
    requires:
      vars: ["cluster"]

  network:
    desc: Create a netshoot container for a cluster
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        ns: Namespace the PVC is in (default: default)
    interactive: true
    requires:
      vars: ["cluster"]
    cmd: |
      kubectl run --context {{.cluster}} -n {{.ns}} netshoot --rm -i --tty --image ghcr.io/nicolaka/netshoot:latest {{.CLI_ARGS}}
    vars:
      ns: '{{.ns | default "default"}}'

  list-dockerhub:
    desc: What dockerhub images are running in my cluster
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    interactive: true
    cmd: |
      kubectl --context {{.cluster}} get pods --all-namespaces -o=jsonpath="{range .items[*]}{'\n'}{range .spec.containers[*]}{.image}{'\n'}{end}{end}" | sort | uniq | grep -Ev 'quay|gcr|ghcr|ecr|us-docker|registry.k8s' | grep -Ev 'bitnami|rook|intel|grafana' |  sed -e 's/docker\.io\///g' | sort | uniq
    requires:
      vars: ["cluster"]

  hash-secret:
    desc: Hash a client secret (for Authelia)
    interactive: true
    cmd: podman run --rm -ti authelia/authelia:latest authelia crypto hash generate pbkdf2 --variant sha512

  letmeout:
    desc: run a privileged container with host access on a node
    summary: |
      Args:
        node: The node (required)
    interactive: true
    cmd: |
      kubectl run letmeout-{{.node}} -n kube-system --rm -i --tty --image=archlinux --overrides='
        {
          "apiVersion": "v1",
          "spec": {
            "containers": [
              {
                "name": "arch",
                "image": "archlinux",
                "args": [
                  "bash"
                ],
                "securityContext": {
                  "allowPrivilegeEscalation": true,
                  "privileged": true
                },
                "stdin": true,
                "stdinOnce": true,
                "tty": true,
                "volumeMounts": [
                  {
                    "mountPath": "/host",
                    "name": "host"
                  },
                  {
                    "name": "efivars",
                    "mountPath": "/sys/firmware/efi/efivars"
                  },
                  {
                    "name": "run-containerd",
                    "mountPath": "/run/containerd"
                  }
                ]
              }
            ],
            "dnsPolicy": "ClusterFirstWithHostNet",
            "hostIPC": true,
            "hostNetwork": true,
            "hostPID": true,
            "nodeName": "{{.node}}",
            "restartPolicy": "Never",
            "volumes": [
              {
                "name": "host",
                "hostPath": {
                  "path": "/"
                }
              },
              {
                "name": "efivars",
                "hostPath": {
                  "path": "/sys/firmware/efi/efivars"
                }
              },
              {
                "name": "run-containerd",
                "hostPath": {
                  "path": "/run/containerd"
                }
              }
            ]
          }
        }'
    requires:
      vars: ["node"]
    preconditions:
      - kubectl get node {{.node}}

  # https://docs.github.com/en/enterprise-cloud@latest/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller#upgrading-arc
  upgrade-arc:
    desc: Upgrade the ARC
    cmds:
      - helm -n actions-runner-system uninstall gha-runner-scale-set
      - helm -n actions-runner-system uninstall java-runner
      - helm -n actions-runner-system uninstall gha-runner-scale-set-controller
      - sleep 5
      - flux -n actions-runner-system reconcile hr gha-runner-scale-set-controller
      - flux -n actions-runner-system reconcile hr gha-runner-scale-set
      - flux -n actions-runner-system reconcile hr java-runner
    preconditions:
      - which flux helm

