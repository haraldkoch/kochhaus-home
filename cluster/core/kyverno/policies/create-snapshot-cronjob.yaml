---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: create-snapshot-cronjob
  annotations:
    policies.kyverno.io/title: Create snapshot cronjob
    policies.kyverno.io/subject: Deployment,StatefulSet
    policies.kyverno.io/description: >-
      This policy creates a snapshot CronJob for Deployments or
      StatefulSets that contain the 'pmb.home.arpa/snapshot-claim' label
spec:
  generateExistingOnPolicyUpdate: true
  mutateExistingOnPolicyUpdate: true
  rules:
    - name: create-snapshot-cronjob
      match:
        any:
          - resources:
              kinds: [Deployment, StatefulSet]
              selector:
                matchLabels:
                  pmb.home.arpa/snapshot-claim: "*"
      context:
        - name: appName
          variable:
            jmesPath: request.object.metadata.name
        - name: claimName
          variable:
            jmesPath: request.object.metadata.labels."pmb.home.arpa/snapshot-claim"
      generate:
        synchronize: true
        apiVersion: batch/v1
        kind: CronJob
        name: "{{ appName }}-snapshot"
        namespace: "{{ request.object.metadata.namespace }}"
        data:
          metadata:
            ownerReferences:
              - apiVersion: "{{ request.object.apiVersion }}"
                kind: "{{ request.object.kind }}"
                name: "{{ request.object.metadata.name }}"
                uid: "{{ request.object.metadata.uid }}"
          spec:
            schedule: "0 3 * * *"
            suspend: false
            concurrencyPolicy: Forbid
            successfulJobsHistoryLimit: 1
            failedJobsHistoryLimit: 2
            jobTemplate:
              spec:
                ttlSecondsAfterFinished: 86400
                template:
                  metadata:
                    labels:
                      setGateway: "false"
                  spec:
                    automountServiceAccountToken: false
                    restartPolicy: OnFailure
                    # Stagger jobs to run randomly within X seconds to avoid bringing down all apps at once
                    initContainers:
                      - name: wait-for
                        image: ghcr.io/onedr0p/alpine:3.16.2@sha256:35dc08b62b80ebd6d9a4e46ff1c4503967d4c0ca6a9ca8b04e299102b0319bf6
                        command: ["/scripts/sleep.sh"]
                        args: ["1", "900"]
                    containers:
                      - name: snapshot
                        image: ghcr.io/onedr0p/kopia:0.12.0@sha256:08f661428342edeac13866e14455cd044624ebc91990fa73e74619d2fbcee8ed
                        env:
                          - name: KOPIA_CACHE_DIRECTORY
                            value: /data/snapshots/{{ appName }}/cache
                          - name: KOPIA_LOG_DIR
                            value: /data/snapshots/{{ appName }}/logs
                          - name: KOPIA_PASSWORD
                            value: "none"
                          - name: KUBEXIT_NAME
                            value: backup-job
                          - name: KUBEXIT_GRAVEYARD
                            value: /kubexit
                          - name: KUBEXIT_POD_NAME
                            valueFrom:
                              fieldRef:
                                fieldPath: metadata.name
                          - name: KUBEXIT_NAMESPACE
                            valueFrom:
                              fieldRef:
                                fieldPath: metadata.namespace
                        command:
                          - /bin/bash
                          - -c
                          - |-
                            printf "\e[1;32m%-6s\e[m\n" "[01/09] Create repo ..."             && [[ ! -f /data/snapshots/kopia.repository.f ]] && kopia repository create filesystem --path=/data/snapshots
                            printf "\e[1;32m%-6s\e[m\n" "[02/09] Connect to repo ..."         && kopia repo connect filesystem --path=/data/snapshots --override-hostname=cluster --override-username=root
                            printf "\e[1;32m%-6s\e[m\n" "[03/09] Set policies ..."            && kopia policy set /{{ appName }} --compression=zstd --keep-latest 14 --keep-hourly 0 --keep-daily 7 --keep-weekly 2 --keep-monthly 0 --keep-annual 0
                            printf "\e[1;32m%-6s\e[m\n" "[04/09] Freeze /{{ appName }} ..."   && fsfreeze -f /{{ appName }}
                            printf "\e[1;32m%-6s\e[m\n" "[05/09] Snapshot /{{ appName }} ..." && kopia snap create /{{ appName }}
                            printf "\e[1;32m%-6s\e[m\n" "[06/09] Unfreeze /{{ appName }} ..." && fsfreeze -u /{{ appName }}
                            printf "\e[1;32m%-6s\e[m\n" "[07/09] List snapshots ..."          && kopia snap list /{{ appName }}
                            printf "\e[1;32m%-6s\e[m\n" "[08/09] Show stats ..."              && kopia content stats
                            printf "\e[1;32m%-6s\e[m\n" "[09/09] Disconnect from repo ..."    && kopia repo disconnect
                        volumeMounts:
                          - name: config
                            mountPath: "/{{ appName }}"
                          - name: snapshots
                            mountPath: /data/snapshots
                        securityContext:
                          privileged: true
                    volumes:
                      - name: config
                        persistentVolumeClaim:
                          claimName: "{{ claimName }}"
                      - name: snapshots
                        nfs:
                          server: ${SECRET_BACKUP_SERVER}
                          path: /backup/data/k8s/kopia
                    affinity:
                      podAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                          - labelSelector:
                              matchExpressions: "{{ items(request.object.spec.selector.matchLabels, 'key', 'value')[].{key: key, operator: 'In', values: [value] } }}"
                            topologyKey: kubernetes.io/hostname
